Machine Learning- HomeAssingmnet 5
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
University Of Central Missouri 

Student Information

Name: Nikhilesh Katakam 

700#: 700772365

Course: CS5710 ‚Äî Machine Learning Semester Fall 2025 

Assignment: Home Assignment 5
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üìåOverview 
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

This repository contains my solutions for Home Assignment 5, which covers two main components:

Part A ‚Äî Short-Answer Questions Focused on Transformers, Positional Encoding, Attention Mechanisms, Multi-Head Attention, Ethics in AI, Dataset Bias, and AI-related Harms.

Part B ‚Äî Coding Tasks

Q1: Implementation of Scaled Dot-Product Attention using NumPy

Q2: Implementation of a Simple Transformer Encoder Block using PyTorch

Includes Multi-Head Self-Attention, Feed-Forward Network, LayerNorm, Residuals, and Shape Verification

All code is fully commented as required and structured for clarity.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

üìçPart A ‚Äî Short Answer Summary

The written answers include detailed explanations on:

‚úî Positional Encoding

Why Transformers need positional encodings

Requirements for good encoding schemes

Unitary & norm-preserving positional matrices

‚úî Attention Mechanism

Definition of attention scores

Role of softmax

Computation of context vectors

‚úî Multi-Head Attention

Advantages of multiple heads

Subspace projection

Why concatenation + linear projection is needed

‚úî Ethics in AI

Difference between ethics, laws, and feelings

Utilitarian vs Deontological reasoning in AI decisions

Why no single ethical theory dominates

‚úî AI Harms

Allocational vs representational harms

Real-world examples

Why representation harms are harder to measure

‚úî Dataset Bias

Sources of bias

Underrepresented groups

Bias amplification

‚úî Security & Privacy

Data poisoning

Model memorization

Model stealing

üßë‚Äçüíª Part B ‚Äî Coding Q1 ‚Äî Scaled Dot-Product Attention (NumPy)

This script implements:

Stable softmax

Scaled dot-product attention

Optional masking

Test demonstrating correct output shapes

Q2 ‚Äî Transformer Encoder Block (PyTorch)

This script includes:

Multi-Head Self-Attention implemented manually

Feed-Forward Network

Add & Norm layers

Residual connections

Dropout

Shape verification test

‚öôÔ∏è Dependencies

Ensure you have the following installed:

Python Libraries pip install numpy torch

Versions used during development:

Python 3.10

NumPy 1.26+

PyTorch 2.0+
