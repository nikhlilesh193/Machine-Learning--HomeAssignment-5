{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyReDjgCWPx15CCe899yjk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4LoT8i3ya0uT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDrHm4uv4Kjn","executionInfo":{"status":"ok","timestamp":1763510605634,"user_tz":360,"elapsed":285,"user":{"displayName":"Nikhilesh Katakam","userId":"16855013178947854755"}},"outputId":"f81ee4a7-249e-4051-afc7-8fcd15ecc284"},"outputs":[{"output_type":"stream","name":"stdout","text":["attn_w shape: (2, 3, 4)\n","ctx shape: (2, 3, 6)\n"]}],"source":["# Q1_scaled_attention.py\n","import numpy as np\n","\n","def softmax(x, axis=-1):\n","    # numerically stable softmax\n","    x_max = np.max(x, axis=axis, keepdims=True)\n","    e = np.exp(x - x_max)\n","    return e / np.sum(e, axis=axis, keepdims=True)\n","\n","def scaled_dot_product_attention(Q, K, V, mask=None):\n","    \"\"\"\n","    Compute scaled dot-product attention.\n","    Inputs:\n","      Q: (batch, seq_q, d_k)\n","      K: (batch, seq_k, d_k)\n","      V: (batch, seq_k, d_v)\n","      mask: optional (batch, seq_q, seq_k) with 0 for allowed positions and -inf (or large negative) for masked\n","    Returns:\n","      attention_weights: (batch, seq_q, seq_k)\n","      context: (batch, seq_q, d_v)\n","    \"\"\"\n","    d_k = Q.shape[-1]\n","    # raw scores: (batch, seq_q, seq_k)\n","    scores = np.matmul(Q, np.swapaxes(K, -1, -2)) / np.sqrt(d_k)\n","    if mask is not None:\n","        # assume mask contains True for positions to mask, or additive mask\n","        scores = np.where(mask, -1e9, scores)\n","    # attention weights\n","    attn_weights = softmax(scores, axis=-1)\n","    # context: weighted sum over values\n","    context = np.matmul(attn_weights, V)  # (batch, seq_q, d_v)\n","    return attn_weights, context\n","\n","# small test\n","if __name__ == \"__main__\":\n","    np.random.seed(1)\n","    B = 2\n","    seq_q = 3\n","    seq_k = 4\n","    d_k = 8\n","    d_v = 6\n","    Q = np.random.randn(B, seq_q, d_k)\n","    K = np.random.randn(B, seq_k, d_k)\n","    V = np.random.randn(B, seq_k, d_v)\n","    attn_w, ctx = scaled_dot_product_attention(Q, K, V)\n","    print(\"attn_w shape:\", attn_w.shape)  # (2,3,4)\n","    print(\"ctx shape:\", ctx.shape)        # (2,3,6)\n"]},{"cell_type":"code","source":["# Q2_transformer_encoder.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleMultiHeadSelfAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_head = d_model // num_heads\n","\n","        # linear projections for Q, K, V and output\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x, mask=None):\n","        # x: (batch, seq_len, d_model)\n","        B, T, _ = x.size()\n","        Q = self.W_q(x)  # (B, T, d_model)\n","        K = self.W_k(x)\n","        V = self.W_v(x)\n","\n","        # reshape to heads: (B, num_heads, T, d_head)\n","        def split_heads(tensor):\n","            return tensor.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n","        Qh = split_heads(Q)\n","        Kh = split_heads(K)\n","        Vh = split_heads(V)\n","\n","        # scaled dot-product per head\n","        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / (self.d_head ** 0.5)  # (B, h, T, T)\n","        if mask is not None:\n","            # mask shape should be broadcastable to (B, 1, T, T), use True for masked positions\n","            scores = scores.masked_fill(mask.unsqueeze(1).bool(), float('-inf'))\n","        attn = F.softmax(scores, dim=-1)  # (B, h, T, T)\n","        out_heads = torch.matmul(attn, Vh)  # (B, h, T, d_head)\n","\n","        # concat heads -> (B, T, d_model)\n","        out = out_heads.transpose(1, 2).contiguous().view(B, T, self.d_model)\n","        out = self.W_o(out)\n","        return out, attn  # return attention for debugging if desired\n","\n","class TransformerEncoderBlock(nn.Module):\n","    def __init__(self, d_model=128, num_heads=8, d_ff=512, dropout=0.1):\n","        super().__init__()\n","        self.self_attn = SimpleMultiHeadSelfAttention(d_model, num_heads)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.ReLU(),\n","            nn.Linear(d_ff, d_model)\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        # x: (B, T, d_model)\n","        attn_out, attn = self.self_attn(x, mask=mask)\n","        x = x + self.dropout(attn_out)    # residual + dropout\n","        x = self.norm1(x)\n","\n","        ffn_out = self.ffn(x)\n","        x = x + self.dropout(ffn_out)     # residual + dropout\n","        x = self.norm2(x)\n","\n","        return x, attn\n","\n","# test / verification\n","if __name__ == \"__main__\":\n","    B = 32\n","    T = 10\n","    d_model = 128\n","    num_heads = 8\n","    x = torch.randn(B, T, d_model)\n","\n","    block = TransformerEncoderBlock(d_model=d_model, num_heads=num_heads, d_ff=512)\n","    out, attn = block(x)  # out shape should be (32, 10, 128)\n","    print(\"out.shape:\", out.shape)   # expect (32, 10, 128)\n","    print(\"attn.shape:\", attn.shape) # expect (32, num_heads, 10, 10)\n"],"metadata":{"id":"HQZo2qZG5JzR","executionInfo":{"status":"ok","timestamp":1763510634942,"user_tz":360,"elapsed":5620,"user":{"displayName":"Nikhilesh Katakam","userId":"16855013178947854755"}},"outputId":"3d9e83f7-6517-4b44-8ac9-5cadd0fe297f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["out.shape: torch.Size([32, 10, 128])\n","attn.shape: torch.Size([32, 8, 10, 10])\n"]}]}]}